---
title: 'ML7 Optimization: Breaking Codes'
author: "Rex YC WONG"
date: "7 January 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Almost every algorithm in machine learning can be viewed as an optimization problem in which we try to minimize some prediction error. But sometimes our parameters are not simle numbers, and so evaluating your error function doesn't give you enough information about nearby points to use optim. For these problems, we could use grid search, but there are approaches that work better than grid search. We'll focus on one approach that's fairly intuitive and very powerful.

Stochastic optimization is to move through the range of possible parametes slightly randomly, but make sure to head in directions where your error function tends to go down rather than go up.

This approach is related to a lot of popular optimization algorithms such as simulated annealing, genetic algorithms, and Markov chain Monte Carlo (MCMC). The specific algorithm we will use is called Metropolis method; versions of the Metropolis method power a lot of modern machine learning algorithms

To illustrate the Metropolis method, we 'll work through this chapter's case study: breaking secret codes. The algorithm we are going to define isn't very effient decryption system and would never be seriously used for production systems, but it is a clear example of how to use te Metropolis method. It is also an example where most out-of-the-box optimization algorithms such as optim could never work.

The problem: given a string of letters that you know are encrypted using a substitution cipher, how do we decide on a decryption rule that gives us the original text?

To create a Caesar cipher:
```{r}
english.letters <- c('a','b','c','d','e','f','g','h','i','j','k',
                     'l','m','n','o','p','q','r','s','t','u','v',
                     'w','x','y','z')

caesar.cipher <- list()
inverse.caesar.cipher <- list()

for(index in 1:length(english.letters)){
  caesar.cipher[[english.letters[index]]] <- english.letters[index%%26+1]
  inverse.caesar.cipher[[english.letters[index%%26+1]]] <- english.letters[index]
}

print(caesar.cipher)
print(inverse.caesar.cipher)
```

Build some functions so that we can translate a string using a cipher

```{r}
apply.cipher.to.string <- function(string,cipher){
  output <- ''
  for(i in 1 : nchar(string)){
    output <- paste(output,cipher[[substr(string,i,i)]],sep='')
  }
  return(output)
}

apply.cipher.to.text <- function(text,cipher){
  output <- c()
  for(string in text){
    output <- c(output,apply.cipher.to.string(string,cipher))
  }
  return(output)
}

apply.cipher.to.text(c('sample','text'),caesar.cipher)
```


Now we have the basic tools for working with ciphers. Now we start thinking about the problem of breaking the codes we might come across.
Define the problem:
1)  Define a measure of the quality of a proposed decryption rule
2)  Define an algorithm for proposing new potential decryption rules that randomly modifies versions of our current best
3)  Define an algorithm for moving progressively toward better decryption rules

Measure the quality of decryption rule:

To start thinking about how to measure the quality of decryption rule, let's suppose that you were given a piece of text that you knew had been encrypted using a substitution cipher. For example, you only know "wfoi, wjej, wjdj" is "veni vidi vici"

The approach we are going to take here to solving that problem is to say that a rule is goold if it turns the encrypted message into normal English. Given a proposed decryption rule, you will run the rule on our encrypted text and see whether the output is realistic English.

How to make that decision? We need to transform the human intuition into something automatic that we can program a computer to do. We will use a lexical database that provides the probability for any word we see. Real language will be equivalent to text built out of words that have high probability, whereas fake language will be equivalent to text with words that have low probability. The only complexity with this approach is dealing with words that don't exist at all. Because their probability is zero and we are going to estimate the probability of a piece of text as a whole by multiplying the probability of the individual words together, we will need to replace zero wtih something really small, called epsilon.

Once we have handled that edege case, we can use a lexical database to rank the quality of two pieces of translated text by finding the probability of each word and then multiplying these probabilities together to find an estimate of the probability of the text as a whole

Using a lexical database to calculate the probability of the decrypted text will give us our error metric for evaluating a proposed rule. Now that we have an error function, our code-breaking problem has turned entirely into an optimization problem, the object becomes finding decrpytion rules that produce text with high probability.

Defining algorithms:

The problem is finding the rule with the highest text probability isn't close to being the sort of problem where optim would work. Decryption rules can't be graphed and don't have the smoothness that optim needs when it is trying to figure out how to head toward better rules. So we need a totally new optimization algorithm for solving our decrption problem. The algorithm we used is Metropolis method.

The basic idea for the Metropolis method is that we will start with an arbitrary decryption rule and then iteratively imporve it many times so that it becomes a rule that could feasibly be the right one. Once we have a potential decryption rule in hand, we can use our human intuition based on semantic coherence and grammar to decide whether we have correctly decrypted the text.

To generate a good rule, we start with a completely arbitrary rule and then repeat a single operation that improves our rule a large number of times, e.g. 50000 times. Because each step tends to head in the direction of better rules, repeating this operation over and over again will get us somewhere reasonable in the end, though the number of times is purely arbitrary. That's also the reason this algorithm will give you a solution in a reasonable amount of time, and it is very hard to tell if you are moving in the right direction while you are waiting. We are just having a toy example here.

Propose a new decryption rules:
We will do it by randomly disturbing the current rule in just one place. That is we will disturb our current rule by changing the rule's effect on a single letter of the input alphabet.

If "a" is currently translated to "b" under our rule, we will propose a modification that has "a" translate to "q". Because of the way substitution cipher works, this will eventually require another modification to the part of the rule that sent another letter, for example "c" to "q". To keep our cipher working, "c" now has to translate to "b". So our algorithm for proposing new rules becomes making two swaps in our exisiting rule, one randomly selected and another one forced by the definition of a substitution cipher.

If we were naive, we would accept this new proposed rule only if it increased the probability of our decrypted text. That is called gready optimization. Unfortunately, greedy optimization in this case will tend to leave us stuck at bad rules, so we will use the following non-greedy rule to decide between our original rule A and our new rule B instead.

1.  If the probability of the text decrypted with rule B is greater than the probability of the text decrpyted with rule A, then we replace A with B.
2.  If otherwise, we will still replace A with B sometimes, but not every time. To be specific, we will switch over to rule B by prob(T,B)/prob(T,A) percent of the time. It means that we accept rule B more than 0% of the time, which helps us avoid the traps that greedy optimization would have us fall into.

Before we can use the Metropolis method to sort through different cipher, we need some tools for creating the perturbed cipher we have been described:
```{r}
generate.random.cipher <- function(){
  cipher <- list()
  inputs <- english.letters
  outputs <- english.letters[sample(1:length(english.letters),length(english.letters))]
  
  for(index in 1:length(english.letters)){
    cipher[[inputs[index]]] <- outputs[index]
  }
  return(cipher)
}


modify.cipher <- function(cipher,input,output){
  new.cipher <- cipher #the old cipher is not changed a lot, only two places are changed
  new.cipher[[input]] <- output # the new input = the output in the argument
  old.output <- cipher[[input]] #find original outout of the input changed
  collateral.input <- names(which(sapply(names(cipher), #find the original key that give the output
                                         function(key){
                                           cipher[[key]]
                                         })==output))
  new.cipher[[collateral.input]] <- old.output #give the old output to the key that produced the output in the old cipher
  return(new.cipher)
}

propose.modified.cipher <- function(cipher){
  input <- sample(names(cipher),1)
  output <- sample(english.letters,1)
  return(modify.cipher(cipher,input,output))
}
```

Combinin this tool for proposing new rules and the rule-swapping procedure we specified softens the greediness of our optimization approach without making us waste too much time on obvious bad rules that have much lower probability than our current rule.

To do this softening algorithmatically, we just compute prob(T,B)/prob(T,A) and compare it with a random number between 0 and 1. If the resulting random number is higher than prob(T,B)/prob(T,A),we replace our current rule, if not, we stick with the current rule.

In order to compute the probabilities that we keep mentionin, we have created a lexical databease tells you how often each of the words in /usr/share/disc/words occurs in text on Wikipedia.
```{r}
load('data/lexical_database.Rdata')
```

We need some methods to calculate the probaiblity of text. First, we will write a function to wrap pulling the probability from database. Writing a function makes it easier to handle fake words that need to be assigned the lowest possible probability, which is going to be you machine's floating point epsilon. To get access to that value in R, you can use the variable .Machine$double.eps
This function  finds the probability of isolated words 
```{r}
one.gram.probability <- function(one.gram,lexical.database=list()){
  lexical.probability <- lexical.database[[one.gram]]
  if(is.null(lexical.probability)||is.na(lexical.probability)){
    return(.Machine$double.eps)
  }
  else{
    return(lexical.probability)
  }
}
```
We create a method for calculating probability of a piece of text by pulling the text apart into separate words, calculating probabilities in isolation, and putting them back together again by multiplying them together. Unfortunately, it turns out that using raw probabiltiies is numerically unstable because of the finete precision arithmetic that floating point numbers provide when you do multipulication. For that reason, we actually compute the log probability of the text which is just the sum of the log probabilities of each word in the text. That value turns out to be relatively stable.

```{r}
log.probability.of.text <- function(text,cipher,lexical.database=list()){
  log.probability <- 0.0
  for(string in text){
    decrypted.string <- apply.cipher.to.string(string,cipher)
    log.probability <- log.probability+log(one.gram.probability(decrypted.string,lexical.database))
    return(log.probability)
  }
}
```
We can write a single step of the Metropolis method as follow
```{r}
metropolis.step <- function(text,cipher,lexical.database=list()){
  proposed.cipher <- propose.modified.cipher(cipher)
  lp1 <- log.probability.of.text(text,cipher,lexical.database)
  lp2 <- log.probability.of.text(text,proposed.cipher,lexical.database)
  if(lp2>lp1){
    return(proposed.cipher)
  }
  else{
    a <- exp(lp2-lp1)
    x <- runif(1)
    if(x<a){
      return(proposed.cipher)
    }
    else{
      return(cipher)
    }
  }
}
```

We have all the individual steps, we need put them together in a single example program that shows how they work
```{r}
decrypted.text <- c('here','is','some','sample','text')
encrypted.text <- apply.cipher.to.text(decrypted.text,caesar.cipher) #create the cipher

```
From there, we will create a random decryption cipher, run 50000 Metropolis steps, and store the results in a data.frame called results. For each step, we will keep a record of the log probability of the decrypted text, the current decryption of the sample textk and a dummy variable indicating whether we have correctly decrypted the input text.

```{r}
set.seed(1)
cipher <- generate.random.cipher()
results <- data.frame()
number.of.iterations <- 50000
for(iteration in 1:number.of.iterations){
  log.probability <- log.probability.of.text(encrypted.text,cipher,lexical.database)
  current.decrypted.text <- paste(apply.cipher.to.text(encrypted.text,cipher),collapse = ' ')
  correct.text <- as.numeric(current.decrypted.text==paste(decrypted.text,collapse = ' '))
  results <- rbind(results,
                   data.frame(
                     Iteration=iteration,
                     LogProbability=log.probability,
                     CurrentDecryptedText=current.decrypted.text,
                     CorrectText=correct.text
                   ))
  cipher <- metropolis.step(encrypted.text,cipher,lexical.database)
}

results
```